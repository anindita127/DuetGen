<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DuetGen</title>
  <link rel="stylesheet" href="style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="container">
      <div class="logos-container">
          <a href="https://research.snap.com/index.html" target="_blank" rel="noopener noreferrer"><img src="assets/SnapInc.(black).png" alt="Snap Logo"></a>
        <a href="https://www.dfki.de/web/forschung/forschungsbereiche/agenten-und-simulierte-realitaet/" target="_blank" rel="noopener noreferrer"><img src="assets/DFKI_logo.png" alt="DFKI Logo"></a>
        <a href="https://www.mpi-inf.mpg.de/home" target="_blank" rel="noopener noreferrer"><img src="assets/MPI_logo.png" alt="MPI Logo"></a>
        <a href="https://saarland-informatics-campus.de/en/" target="_blank" rel="noopener noreferrer"><img src="assets/SIC.webp" alt="SIC Logo"></a>
      </div>
      <h1>DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling</h1>
      
      <p class="authors">
          <a href="https://anindita127.github.io/">Anindita Ghosh</a><sup>1,2,3</sup>*, <a href="https://zhoubinwy.github.io/">Bing Zhou</a><sup>4</sup>, <a href="https://rishabhdabral.github.io/">Rishabh Dabral</a><sup>2,3</sup>,<a href="https://jianwang-cmu.github.io/">Jian Wang</a><sup>4</sup>, 
          <a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a><sup>2,3</sup>, 
          <br> <a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a><sup>2,3</sup>,
          <a href="https://graphics.cg.uni-saarland.de/people/slusallek.html">Philipp Slusallek</a><sup>1,3</sup>, <a href="https://ericguo5513.github.io/">Chuan Guo</a><sup>4</sup><br> <br> 
          
          <sup>1</sup><a href="https://www.dfki.de/web/forschung/forschungsbereiche/agenten-und-simulierte-realitaet/" target="_blank" rel="noopener noreferrer">German Research Center for Artificial Intelligence (DFKI)</a>,  
          <br><sup>2</sup><a href="https://www.mpi-inf.mpg.de/home" target="_blank" rel="noopener noreferrer">Max Planck Institute for Informatics</a>,  
          <br><sup>3</sup><a href="https://saarland-informatics-campus.de/en/" target="_blank" rel="noopener noreferrer">Saarland Informatics Campus</a>,  
          <br><sup>4</sup><a href="https://research.snap.com/index.html" target="_blank" rel="noopener noreferrer">Snap Inc.</a>
          
          <br> <em>*Work done during internship at Snap Research</em>
    </p>

    <br>
    <p class="announcement">This work is accepted at the SIGGRAPH 2025 technical paper conference track.</p>


    <p align="center">
      <a href='https://arxiv.org/pdf/2506.18680'>
        <img src='https://img.shields.io/badge/Arxiv-Pdf-A42C25?style=flat&logo=arXiv&logoColor=white'></a>
      <a href='https://github.com/anindita127/DuetGen'>
        <img src='https://img.shields.io/badge/GitHub-Code-black?style=flat&logo=github&logoColor=white'></a>
      <a href='https://www.youtube.com/watch?v=yebDWEiOM0s'>
        <img src='https://img.shields.io/badge/YouTube-Video-blue?style=flat&logo=youtube&logoColor=white'></a>
      <a href='#citation'>
        <img src='https://img.shields.io/badge/BibTex-Citation-green?style=flat&logo=latex&logoColor=white'></a>  
    </p>

    
    <img src="assets/project_teaser.jpg" alt="Project Teaser" class="teaser">
    <p class="caption"><b>DuetGen</b> can generate synchronized two-person dance choreography from any input music, featuring natural and close interactions between dancers.</p>

    <h2>Abstract</h2>
    <div style="text-align: justify; font-size: 0.9em;">
        We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination.
        </div>
        

    <h2>Approach</h2>
    <img src="assets/inference_process.jpg" alt="Our model architecture" class="teaser">
    <p class="caption"><b>Overview of our approach. </b>
      Our first-stage transformer \(\theta_t\) iteratively fills an empty sequence of top-level tokens in \(L_{top}\) iterations based on input music. Then,
the second-stage transformer \(\theta_b\) generates the complete sequence of bottom-level tokens in \(L_{bot}\) iterations, conditioned on both music and the generated
top-level tokens \(\hat{t}_{top}\). We combine these token sequences and decode them into two-person dance motions via the VQ-VAE decoder \(D\). We then apply a
lightweight network to refine the root trajectories and produce the final outputs.
    </p>
    

    <h2>âœ¨ Results âœ¨</h2>
    <p class="announcement">ðŸ”Š All videos contain audio.</p>
    <!-- <p class="caption" style="text-align: center;">ðŸ”Š All videos contain audio.</p> -->
    <div class="results-container">
      <video controls autoplay loop muted class="result-vid">
      <source src="assets/Media6.mp4" type="video/mp4">
      </video>
      <video controls autoplay loop muted class="result-vid">
      <source src="assets/Media7.mp4" type="video/mp4">
      </video>
      <video controls autoplay loop muted class="result-vid">
      <source src="assets/Media8.mp4" type="video/mp4">
      </video>
      <video controls autoplay loop muted class="result-vid">
        <source src="assets/Media9.mp4" type="video/mp4">
      </video>
      <video controls autoplay loop muted class="result-vid">
        <source src="assets/Media1.mp4" type="video/mp4">
      </video>
      <video controls autoplay loop muted class="result-vid">
        <source src="assets/Media2.mp4" type="video/mp4">
      </video>
      <video controls autoplay loop muted class="result-vid">
        <source src="assets/Media3.mp4" type="video/mp4">
      </video>
      <video controls autoplay loop muted class="result-vid">
        <source src="assets/Media4.mp4" type="video/mp4">
      </video>
      
    </div>

    
    <h2 id="citation">BibTex</h2>
    <pre>
        @InProceedings{ghosh2025duetgen,
            title={DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling},
            author={Ghosh, Anindita and Zhou, Bing and Dabral, Rishabh and Wang, Jian and Golyanik, Vladislav and 
                    Theobalt, Christian and Slusallek, Philipp and Guo, Chuan},
            booktitle={ACM SIGGRAPH},
            year={2025}
        }
    </pre>
    <div class="section contact">
        <h2>Contact</h2>
        For questions or clarifications, please get in touch with <em>
        <a href="mailto:anghosh@mpi-inf.mpg.de">Anindita Ghosh</a></em>.<br>
        
    </div>
    <footer>
        <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>

            Website source code based on the Nerfies <a href="https://nerfies.github.io/">project page</a>. If you want to reuse their source code, please credit them appropriately.
          </p>
    </footer>
  </div>
</body>
</html>
